{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eleven-serial",
   "metadata": {},
   "source": [
    "# La roue des émotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reported-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "confirmed-client",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\keltarr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\keltarr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\keltarr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "operational-preference",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21454</th>\n",
       "      <td>Melissa stared at her friend in dism</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21455</th>\n",
       "      <td>Successive state elections have seen the gover...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21456</th>\n",
       "      <td>Vincent was irritated but not dismay</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21457</th>\n",
       "      <td>Kendall-Hume turned back to face the dismayed ...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21458</th>\n",
       "      <td>I am dismayed , but not surpris</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Emotion\n",
       "0                                i didnt feel humiliated  sadness\n",
       "1      i can go from feeling so hopeless to so damned...  sadness\n",
       "2       im grabbing a minute to post i feel greedy wrong    anger\n",
       "3      i am ever feeling nostalgic about the fireplac...     love\n",
       "4                                   i am feeling grouchy    anger\n",
       "...                                                  ...      ...\n",
       "21454               Melissa stared at her friend in dism     fear\n",
       "21455  Successive state elections have seen the gover...     fear\n",
       "21456               Vincent was irritated but not dismay     fear\n",
       "21457  Kendall-Hume turned back to face the dismayed ...     fear\n",
       "21458                    I am dismayed , but not surpris     fear\n",
       "\n",
       "[21459 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\keltarr\\Desktop\\Roue des émotions\\Data\\emotions_final.csv', sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-concord",
   "metadata": {},
   "source": [
    "### Classes du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "durable-renewal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         7029\n",
       "sadness     6265\n",
       "anger       2993\n",
       "fear        2652\n",
       "love        1641\n",
       "surprise     879\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-campus",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "japanese-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowers = []\n",
    "for i in range(0, df.shape[0]):\n",
    "    lowers.append(df['Text'][i].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "promising-array",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  i didnt feel humiliated\n",
       "1        i can go from feeling so hopeless to so damned...\n",
       "2         im grabbing a minute to post i feel greedy wrong\n",
       "3        i am ever feeling nostalgic about the fireplac...\n",
       "4                                     i am feeling grouchy\n",
       "                               ...                        \n",
       "21454                 melissa stared at her friend in dism\n",
       "21455    successive state elections have seen the gover...\n",
       "21456                 vincent was irritated but not dismay\n",
       "21457    kendall-hume turned back to face the dismayed ...\n",
       "21458                      i am dismayed , but not surpris\n",
       "Name: Text, Length: 21459, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'] = lowers\n",
    "df['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-wrist",
   "metadata": {},
   "source": [
    "### Tokénization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "loving-catholic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             [i, didnt, feel, humiliated]\n",
       "1        [i, can, go, from, feeling, so, hopeless, to, ...\n",
       "2        [im, grabbing, a, minute, to, post, i, feel, g...\n",
       "3        [i, am, ever, feeling, nostalgic, about, the, ...\n",
       "4                                [i, am, feeling, grouchy]\n",
       "                               ...                        \n",
       "21454         [melissa, stared, at, her, friend, in, dism]\n",
       "21455    [successive, state, elections, have, seen, the...\n",
       "21456          [vincent, was, irritated, but, not, dismay]\n",
       "21457    [kendall-hume, turned, back, to, face, the, di...\n",
       "21458              [i, am, dismayed, ,, but, not, surpris]\n",
       "Name: Text, Length: 21459, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'] = df['Text'].apply(word_tokenize)\n",
    "df['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-chemical",
   "metadata": {},
   "source": [
    "### Suppression des stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tender-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopW = stopwords.words('english')\n",
    "stopW.extend(list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "broad-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sent):\n",
    "    tokens_without_stopW = [word for word in sent if word not in stopW]\n",
    "    return tokens_without_stopW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "seeing-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda sent: remove_stopwords(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-verse",
   "metadata": {},
   "source": [
    "### Stemming et Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "comprehensive-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "taken-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(sent, join=True):\n",
    "    tokens = [wnl.lemmatize(wnl.lemmatize(wnl.lemmatize(w,'v'),'n'),'a') for w in sent]\n",
    "    if join:\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "perceived-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explo = copy.deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "legislative-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda sent: lemmatize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "serious-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explo['Text'] = df_explo['Text'].apply(lambda sent: lemmatize(sent, join=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "moved-stephen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 [didnt, feel, humiliate]\n",
       "1        [go, feel, hopeless, damn, hopeful, around, so...\n",
       "2            [im, grab, minute, post, feel, greedy, wrong]\n",
       "3        [ever, feel, nostalgic, fireplace, know, still...\n",
       "4                                          [feel, grouchy]\n",
       "                               ...                        \n",
       "21454                        [melissa, star, friend, dism]\n",
       "21455    [successive, state, election, see, govern, par...\n",
       "21456                          [vincent, irritate, dismay]\n",
       "21457       [kendall-hume, turn, back, face, dismay, coup]\n",
       "21458                                    [dismay, surpris]\n",
       "Name: Text, Length: 21459, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_explo['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-tuition",
   "metadata": {},
   "source": [
    "### Exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "external-wrong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[d, i, d, n, t,  , f, e, e, l,  , h, u, m, i, ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[g, o,  , f, e, e, l,  , h, o, p, e, l, e, s, ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[i, m,  , g, r, a, b,  , m, i, n, u, t, e,  , ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[e, v, e, r,  , f, e, e, l,  , n, o, s, t, a, ...</td>\n",
       "      <td>love</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[f, e, e, l,  , g, r, o, u, c, h, y]</td>\n",
       "      <td>anger</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21454</th>\n",
       "      <td>[m, e, l, i, s, s, a,  , s, t, a, r,  , f, r, ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21455</th>\n",
       "      <td>[s, u, c, c, e, s, s, i, v, e,  , s, t, a, t, ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21456</th>\n",
       "      <td>[v, i, n, c, e, n, t,  , i, r, r, i, t, a, t, ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21457</th>\n",
       "      <td>[k, e, n, d, a, l, l, -, h, u, m, e,  , t, u, ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21458</th>\n",
       "      <td>[d, i, s, m, a, y,  , s, u, r, p, r, i, s]</td>\n",
       "      <td>fear</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21459 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  Emotion  Length\n",
       "0      [d, i, d, n, t,  , f, e, e, l,  , h, u, m, i, ...  sadness      20\n",
       "1      [g, o,  , f, e, e, l,  , h, o, p, e, l, e, s, ...  sadness      55\n",
       "2      [i, m,  , g, r, a, b,  , m, i, n, u, t, e,  , ...    anger      37\n",
       "3      [e, v, e, r,  , f, e, e, l,  , n, o, s, t, a, ...     love      49\n",
       "4                   [f, e, e, l,  , g, r, o, u, c, h, y]    anger      12\n",
       "...                                                  ...      ...     ...\n",
       "21454  [m, e, l, i, s, s, a,  , s, t, a, r,  , f, r, ...     fear      24\n",
       "21455  [s, u, c, c, e, s, s, i, v, e,  , s, t, a, t, ...     fear      65\n",
       "21456  [v, i, n, c, e, n, t,  , i, r, r, i, t, a, t, ...     fear      23\n",
       "21457  [k, e, n, d, a, l, l, -, h, u, m, e,  , t, u, ...     fear      39\n",
       "21458         [d, i, s, m, a, y,  , s, u, r, p, r, i, s]     fear      14\n",
       "\n",
       "[21459 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_explo['Length'] = df_explo['Text'].apply(len)\n",
    "df_explo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-average",
   "metadata": {},
   "source": [
    "### Classification avec BERTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "virgin-geometry",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-1.0.4.tar.gz (74 kB)\n",
      "Collecting transformers<5.0.0,>=3.1.0\n",
      "  Downloading transformers-4.5.0-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.59.0)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.8.1-cp38-cp38-win_amd64.whl (190.5 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.19.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.24.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.5)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp38-cp38-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.11.13)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.44.tar.gz (862 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
      "Requirement already satisfied: requests in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.25.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.2-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: joblib in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: six in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Building wheels for collected packages: sentence-transformers, sacremoses\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-1.0.4-py3-none-any.whl size=114306 sha256=c8f1f88f145f0513fadd9f3b20739caa02bfb5976330d8b8b5cfbefc8c2622da\n",
      "  Stored in directory: c:\\users\\keltarr\\appdata\\local\\pip\\cache\\wheels\\28\\cb\\ae\\360fe121dc748add4fabecd46e78c31d1ea2402d341f97e2dc\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.44-py3-none-any.whl size=886084 sha256=e240c8736e06d284e881e9e3a8bdb99674b7fd516ac8a31a702b28126092285a\n",
      "  Stored in directory: c:\\users\\keltarr\\appdata\\local\\pip\\cache\\wheels\\d6\\17\\75\\f2ed13c472c4cecc14f003401bb45efadca64cc589d4bf3103\n",
      "Successfully built sentence-transformers sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, transformers, torch, sentencepiece, sentence-transformers\n",
      "Successfully installed sacremoses-0.0.44 sentence-transformers-1.0.4 sentencepiece-0.1.95 tokenizers-0.10.2 torch-1.8.1 transformers-4.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "blind-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "negative-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Emotion'][:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bacterial-happening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce60c3cb26f42a580619478b5e40cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "orange-point",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  i didnt feel humiliated\n",
       "1        i can go from feeling so hopeless to so damned...\n",
       "2         im grabbing a minute to post i feel greedy wrong\n",
       "3        i am ever feeling nostalgic about the fireplac...\n",
       "4                                     i am feeling grouchy\n",
       "                               ...                        \n",
       "21454                 Melissa stared at her friend in dism\n",
       "21455    Successive state elections have seen the gover...\n",
       "21456                 Vincent was irritated but not dismay\n",
       "21457    Kendall-Hume turned back to face the dismayed ...\n",
       "21458                      I am dismayed , but not surpris\n",
       "Name: Text, Length: 21459, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "developed-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(df['Text'][:size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "expensive-eating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "suspected-scope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "postal-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "objective-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings.npy', embeddings) # Save a numpy array in a .npy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "incorporate-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_embedding = np.load('embeddings.npy') # load a numpy array from a .npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "integral-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, y, stratify=y,\n",
    "                                                    test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pressed-startup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\keltarr\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.19.2)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=29529ee7d4fd54746e918797266a02afc155cecfecc1f32d7220972fb0966ae1\n",
      "  Stored in directory: c:\\users\\keltarr\\appdata\\local\\pip\\cache\\wheels\\22\\0b\\40\\fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Note: you may need to restart the kernel to use updated packages.Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "flexible-peter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.66      0.14      0.23       134\n",
      "        fear       0.70      0.12      0.20       117\n",
      "         joy       0.56      0.88      0.69       340\n",
      "        love       0.50      0.01      0.02        80\n",
      "     sadness       0.53      0.76      0.62       289\n",
      "    surprise       0.00      0.00      0.00        40\n",
      "\n",
      "    accuracy                           0.55      1000\n",
      "   macro avg       0.49      0.32      0.30      1000\n",
      "weighted avg       0.55      0.55      0.47      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keltarr\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\keltarr\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\keltarr\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "enhanced-commander",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         7029\n",
       "sadness     6265\n",
       "anger       2993\n",
       "fear        2652\n",
       "love        1641\n",
       "surprise     879\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-church",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
